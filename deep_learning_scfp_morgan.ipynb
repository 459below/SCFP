{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAsrEAjADT_0"
   },
   "source": [
    "Showing the instance ID and uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "fgmTYdJh57ZL",
    "outputId": "747a8816-42a6-4db4-c54c-1e43148e6bbd"
   },
   "outputs": [],
   "source": [
    "!hostname\n",
    "!uptime\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO-YoXAtDZxl"
   },
   "source": [
    "Creating unique experiment ID to not overwrite our results every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XH5o042sNxZt",
    "outputId": "83cf7151-b32d-448a-d8ce-ddeea41841be"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %tensorflow_version 2.x\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "# !time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# !time conda install -q -y -c conda-forge rdkit\n",
    "# sys.path.append('/usr/local/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoySPcI5DPWm"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# !rm -rf ./logs/\n",
    "# cur_date=datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "EXPERIMENT_ID = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "# EXPERIMENT_PATH = '/content/drive/My Drive/Colab Notebooks/experiments'\n",
    "EXPERIMENT_PATH = 'experiments'\n",
    "!mkdir -p {'%s/%s/weights' % (EXPERIMENT_PATH, EXPERIMENT_ID)}\n",
    "print('EXPERIMENT_ID is: %s' % EXPERIMENT_ID)\n",
    "print('Log folder is: %s/%s/logs' % (EXPERIMENT_PATH, EXPERIMENT_ID))\n",
    "print('Weights folder is: %s/%s/weights' % (EXPERIMENT_PATH, EXPERIMENT_ID))\n",
    "print('Model file is: %s/%s/model.h5' % (EXPERIMENT_PATH, EXPERIMENT_ID))\n",
    "print('History file is: %s/%s/history.pickle' % (EXPERIMENT_PATH, EXPERIMENT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "dnTtzSwzPPwQ",
    "outputId": "04b8dd07-0329-495b-b793-71b2bde15983"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import rdkit\n",
    "print(\"Python version: \"+sys.version)\n",
    "print(\"Numpy version: \"+np.__version__)\n",
    "print(\"Tensorflow version: \"+tf.__version__)\n",
    "print(\"Keras version: \"+tf.keras.__version__)\n",
    "print(\"RDKit version: \"+rdkit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "OwwA35uquYJg",
    "outputId": "ba616734-3f18-4ff2-f868-5a74400ca695"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmPXC09RGejr"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 459\n",
    "\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_nets = ['dnn', 'scfp']\n",
    "\n",
    "properties = [\n",
    "            'ahr',\n",
    "            'are',\n",
    "            'ar-lbd',\n",
    "            'aromatase',\n",
    "            'ar',\n",
    "            'atad5',\n",
    "            'er-lbd',\n",
    "            'er',\n",
    "            'hse',\n",
    "            'mmp',\n",
    "            'p53',\n",
    "            'ppar'\n",
    "            ]\n",
    "\n",
    "samplings = ['none', 'over', 'under']\n",
    "evaluations = ['train', 'internal', 'external']\n",
    "metrics = ['accuracy', 'roc_auc', 'recall', 'specificity', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45yGPx0GPp1Q"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "\n",
    "# import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, GlobalMaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bX7QWocX24DU"
   },
   "outputs": [],
   "source": [
    "TOX21_ALL_SDF_TRAIN_URL = \"https://tripod.nih.gov/tox21/challenge/download?id=tox21_10k_data_allsdf\"\n",
    "TOX21_ALL_SDF_TEST_URL = \"https://tripod.nih.gov/tox21/challenge/download?id=tox21_10k_challenge_testsdf\"\n",
    "tox21_all_sdf_train_file_path = tf.keras.utils.get_file(\"tox21_10k_data_all.sdf.zip\", TOX21_ALL_SDF_TRAIN_URL, extract=True)\n",
    "tox21_all_sdf_test_file_path = tf.keras.utils.get_file(\"tox21_10k_challenge_test.sdf.zip\", TOX21_ALL_SDF_TEST_URL, extract=True)\n",
    "tox21_all_sdf_train_file_path = tox21_all_sdf_train_file_path[:-4] # delete file suffix '.zip'\n",
    "tox21_all_sdf_test_file_path = tox21_all_sdf_test_file_path[:-4] # delete file suffix '.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTddVc69B_O4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smiles_dnn.feature import mol_to_feature\n",
    "\n",
    "atomsize=400\n",
    "\n",
    "def preprocess_data(suppl, indices_raw, property):\n",
    "\n",
    "    y = []\n",
    "    errors = 0\n",
    "    good_indices = []\n",
    "    bad_indices = []\n",
    "\n",
    "    for index_raw in indices_raw:\n",
    "        try:\n",
    "            if neural_net == 'dnn':\n",
    "                fp = AllChem.GetMorganFingerprintAsBitVect(suppl[index_raw], 2, nBits=2048)\n",
    "            elif neural_net == 'scfp':\n",
    "                if len(Chem.MolToSmiles(suppl[index_raw], kekuleSmiles=True, isomericSmiles=True)) > atomsize:\n",
    "                    raise Exception\n",
    "                mol_to_feature(suppl[index_raw], -1, atomsize) # Checking wether conversion is possible\n",
    "\n",
    "            y.append(suppl[index_raw].GetBoolProp(property))\n",
    "            good_indices.append(index_raw) # If we made it this far, x and y can be generated for index_raw\n",
    "\n",
    "        except Exception as inst:\n",
    "#             print(type(inst))\n",
    "#             print(inst.args)\n",
    "#             print(inst)\n",
    "\n",
    "            bad_indices.append(index_raw)\n",
    "            errors+=1\n",
    "\n",
    "    print('%i errors occured' % errors)\n",
    "    \n",
    "    return good_indices, y, bad_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCFP architecture\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "featuresize = 42\n",
    "\n",
    "def get_scfp_net():\n",
    "    model = Sequential()\n",
    "    with tf.name_scope('Input'):\n",
    "        model.add(Conv2D(filters=128, kernel_size=(1, featuresize), strides=1, input_shape=(atomsize, featuresize, 1), use_bias=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "    with tf.name_scope('generate_scfp'):\n",
    "        model.add(AveragePooling2D(pool_size=(5, 1), strides=1, padding='same'))\n",
    "        model.add(Conv2D(filters=64, kernel_size=(1, 1), strides=1, use_bias=False))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(AveragePooling2D(pool_size=(5, 1), strides=1, padding='same'))\n",
    "\n",
    "#         model.add(Conv1D(filters=320, kernel_size=5, strides=1, input_shape=(atomsize, featuresize), activation='relu'))\n",
    "#         model.add(AveragePooling1D(pool_size=5, strides=1, padding='same'))\n",
    "#         model.add(Conv1D(filters=170, kernel_size=15, strides=1, activation='relu'))\n",
    "#         model.add(AveragePooling1D(pool_size=5, strides=1, padding='same'))\n",
    "\n",
    "\n",
    "#     with tf.name_scope('Dropout'):\n",
    "#         model.add(Dropout(0.2))\n",
    "    with tf.name_scope('GlobalMaxPooling'):\n",
    "        model.add(GlobalMaxPooling2D())\n",
    "    with tf.name_scope('Hidden'):\n",
    "        model.add(Dense(264, activation='relu'))\n",
    "    with tf.name_scope('Output'):\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def fit_scfp_model(suppl, x_train_indices, y_train, x_val_indices, y_val, hparams, callbacks):\n",
    "\n",
    "    model = get_scfp_net()\n",
    "\n",
    "    model.compile(\n",
    "        loss=hparams[HP_LOSS_FUNCTION],\n",
    "        optimizer=hparams[HP_OPTIMIZER],\n",
    "        metrics=['accuracy', 'AUC', specificity_m, 'Recall', f1_score_m],\n",
    "    )\n",
    "\n",
    "    model.optimizer.lr = hparams[HP_LEARNING_RATE]\n",
    "    model.optimizer.decay = hparams[HP_DECAY]\n",
    "    model.optimizer.momentum = hparams[HP_MOMENTUM]\n",
    "    model.optimizer.clipnorm = 1.0\n",
    "        \n",
    "#     generator = SCFPFeatureMatrixGenerator(x_set=x_train_indices,\n",
    "#                                        y_set=y_train,\n",
    "#                                        batch_size=hparams[HP_BATCH_SIZE],\n",
    "#                                        suppl=suppl)\n",
    "\n",
    "#     generator_validation = SCFPFeatureMatrixGenerator(x_set=x_val_indices,\n",
    "#                                                   y_set=y_val,\n",
    "#                                                   batch_size=hparams[HP_BATCH_SIZE],\n",
    "#                                                   suppl=suppl)\n",
    "\n",
    "    x_train = [ mol_to_feature(suppl[id], -1, atomsize) for id in x_train_indices ]\n",
    "    x_val = [ mol_to_feature(suppl[id], -1, atomsize) for id in x_val_indices ]\n",
    "\n",
    "    x_train = np.asarray(x_train, dtype=np.float32)\n",
    "    x_val = np.asarray(x_val, dtype=np.float32)\n",
    "    \n",
    "    y_train = np.asarray(y_train, dtype=np.int32)\n",
    "    y_val = np.asarray(y_val, dtype=np.int32)\n",
    "    \n",
    "    x_train = x_train.reshape(-1, atomsize, featuresize, 1)\n",
    "    x_val = x_val.reshape(-1, atomsize, featuresize, 1)\n",
    "\n",
    "    \n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "#                         x=generator,\n",
    "#                         validation_data=generator_validation,\n",
    "                        epochs=hparams[HP_EPOCHS],\n",
    "#                         initial_epoch=initial_epoch,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=2\n",
    "                       )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn(hparams, output_bias):\n",
    "    model = Sequential()\n",
    "\n",
    "    with tf.name_scope('Input'):\n",
    "        model.add(Dropout(hparams[HP_DROPOUT_INPUT], input_shape=(2048,))) # HARD CODED VALUE for fingerprint dimensions\n",
    "        model.add(Dense(\n",
    "#             input_dim=x_train.shape[1],\n",
    "            hparams[HP_NUM_UNITS],\n",
    "            kernel_regularizer=l2(hparams[HP_L2_L]),\n",
    "            bias_regularizer=l2(hparams[HP_L2_L])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    with tf.name_scope('Hidden'):\n",
    "        for layer in range(hparams[HP_NUM_HIDDEN_LAYERS]): # Dynamically adding hidden layers with a subsequent dropout\n",
    "            model.add(Dense(\n",
    "                hparams[HP_NUM_UNITS],\n",
    "                kernel_regularizer=l2(hparams[HP_L2_L]),\n",
    "                bias_regularizer=l2(hparams[HP_L2_L])))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    with tf.name_scope('Output'):\n",
    "        model.add(Dense(\n",
    "            1,\n",
    "            kernel_regularizer=l2(hparams[HP_L2_L]),\n",
    "            bias_regularizer=l2(hparams[HP_L2_L]),\n",
    "            bias_initializer=output_bias))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('sigmoid'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def fit_dnn_model(suppl, x_train_indices, y_train, x_val_indices, y_val, hparams, callbacks):\n",
    "\n",
    "    negatives = np.array(y_train).flatten().tolist().count(0)\n",
    "    positives = np.array(y_train).flatten().tolist().count(1)\n",
    "    \n",
    "    output_bias = tf.keras.initializers.Constant(np.log([positives/negatives]))\n",
    "    \n",
    "    model = get_dnn(hparams, output_bias)\n",
    "\n",
    "    model.compile(\n",
    "        loss=hparams[HP_LOSS_FUNCTION],\n",
    "        optimizer=hparams[HP_OPTIMIZER],\n",
    "        metrics=['accuracy', 'AUC', specificity_m, 'Recall', f1_score_m],\n",
    "    )\n",
    "\n",
    "    model.optimizer.nesterov = True\n",
    "    model.optimizer.lr = hparams[HP_LEARNING_RATE]\n",
    "    model.optimizer.decay = hparams[HP_DECAY]\n",
    "    model.optimizer.momentum = hparams[HP_MOMENTUM]\n",
    "    model.optimizer.clipnorm = 1.0\n",
    "\n",
    "    x_train = np.asarray([ AllChem.GetMorganFingerprintAsBitVect(suppl[id], 2, nBits=2048) for id in x_train_indices ])\n",
    "    x_val = np.asarray([ AllChem.GetMorganFingerprintAsBitVect(suppl[id], 2, nBits=2048) for id in x_val_indices ])\n",
    "\n",
    "#     x_train = np.asarray(x_train, dtype=np.int32)\n",
    "#     x_val = np.asarray(x_val, dtype=np.int32)\n",
    "    \n",
    "#     y_train = np.asarray(y_train, dtype=np.int32)\n",
    "#     y_val = np.asarray(y_val, dtype=np.int32)\n",
    "    \n",
    "#     x_train = x_train.reshape(-1, atomsize, featuresize, 1)\n",
    "#     x_val = x_val.reshape(-1, atomsize, featuresize, 1)\n",
    "\n",
    "    \n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "#                         x=generator,\n",
    "#                         validation_data=generator_validation,\n",
    "                        epochs=hparams[HP_EPOCHS],\n",
    "#                         initial_epoch=initial_epoch,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=2\n",
    "                       )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, classification_report, make_scorer, recall_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "\n",
    "HP_EPOCHS = hp.HParam('epochs', hp.Discrete([250]))\n",
    "# HP_EPOCHS = hp.HParam('epochs', hp.Discrete([40]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32])) # We use minibatching, because it will converge faster\n",
    "HP_LOSS_FUNCTION = hp.HParam('loss_function', hp.Discrete(['binary_crossentropy']))\n",
    "# HP_LOSS_FUNCTION = hp.HParam('loss_function', hp.Discrete(['masked_loss_function']))\n",
    "# HP_LOSS_FUNCTION = hp.HParam('loss_function', hp.Discrete(['binary_crossentropy', 'mse', 'masked_loss_function']))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['Adam']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['SGD', 'RMSProp', 'Adam']))\n",
    "# HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-1]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-3]))\n",
    "HP_DECAY = hp.HParam('decay', hp.Discrete([1e-6]))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.Discrete([0.9]))\n",
    "HP_L2_L = hp.HParam('l2_l', hp.Discrete([1e-4]))\n",
    "\n",
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([1024]))\n",
    "HP_NUM_HIDDEN_LAYERS = hp.HParam('num_hidden_layers', hp.Discrete([2]))\n",
    "HP_DROPOUT_INPUT = hp.HParam('dropout_input', hp.Discrete([0.2]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.5]))\n",
    "\n",
    "# properties = ['NR-AhR']\n",
    "# properties = ['NR-ER-LBD']\n",
    "# properties = ['NR-AhR', 'NR-ER-LBD']\n",
    "# properties = ['NR-AhR', 'NR-AR', 'NR-AR-LBD', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53', 'NR-Aromatase']\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for neural_net in neural_nets:\n",
    "    results[neural_net] = {}\n",
    "    for property in properties:\n",
    "        results[neural_net][property] = {}\n",
    "        for sampling in samplings:\n",
    "            results[neural_net][property][sampling] = {}\n",
    "            for evaluation in evaluations:\n",
    "                results[neural_net][property][sampling][evaluation] = {}\n",
    "                for metric in metrics:\n",
    "                    results[neural_net][property][sampling][evaluation][metric] = []\n",
    "\n",
    "#     !echo {properties} > {\"%s/%s/properties.txt\" % (EXPERIMENT_PATH, EXPERIMENT_ID)}\n",
    "print(properties)\n",
    "\n",
    "\n",
    "\n",
    "k = 10\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "\n",
    "for neural_net in neural_nets:\n",
    "    for property in properties:\n",
    "\n",
    "        suppl = Chem.SDMolSupplier('data/curated/%s.sdf' % property)\n",
    "\n",
    "        x_indices, y, bad_indices = preprocess_data(suppl, range(len(suppl)), property)\n",
    "\n",
    "    #     negatives = np.array(y).flatten().tolist().count(0)\n",
    "    #     positives = np.array(y).flatten().tolist().count(1)\n",
    "    #     print('Percentage of positives: %f' % (positives/negatives))\n",
    "\n",
    "        print(\"Hash of indices list %s\" % (hex(hash(frozenset(x_indices)))))\n",
    "\n",
    "        # first split into training + validation (90%) and test (10%)\n",
    "        x_train_val_indices, x_test_indices, y_train_val, y_test = train_test_split(x_indices, y, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "        print(\"Hash of indices list - train_val: %s, test: %s\" % (hex(hash(frozenset(x_train_val_indices))), hex(hash(frozenset(x_test_indices)))))\n",
    "\n",
    "    #     negatives = np.array(y_train_val).flatten().tolist().count(0)\n",
    "    #     positives = np.array(y_train_val).flatten().tolist().count(1)\n",
    "    #     print('Percentage of positives in train_val: %f' % (positives/negatives))\n",
    "\n",
    "    #     negatives = np.array(y_test).flatten().tolist().count(0)\n",
    "    #     positives = np.array(y_test).flatten().tolist().count(1)\n",
    "    #     print('Percentage of positives in test: %f' % (positives/negatives))\n",
    "\n",
    "        # Double checking wether our data sets are correctly disjoint\n",
    "        if not set(frozenset(x_train_val_indices)).isdisjoint(frozenset(x_test_indices)):\n",
    "            raise Exception\n",
    "\n",
    "        for fold, (train_indices_of_kfold, val_indices_of_kfold) in enumerate(kfold.split(X=x_train_val_indices, y=y_train_val)):\n",
    "\n",
    "            for sampling in samplings:\n",
    "                \n",
    "                print(\"Doing neural net %s with property %s with sampling %s in fold %i\" % (neural_net, property, sampling, fold))\n",
    "\n",
    "                x_train_indices = np.array(x_train_val_indices)[train_indices_of_kfold.astype(int)]\n",
    "                x_val_indices = np.array(x_train_val_indices)[val_indices_of_kfold.astype(int)]\n",
    "\n",
    "                y_train = np.array(y_train_val)[train_indices_of_kfold.astype(int)]\n",
    "                y_val = np.array(y_train_val)[val_indices_of_kfold.astype(int)]\n",
    "\n",
    "                # Sampling the data\n",
    "\n",
    "                if sampling == 'over':\n",
    "                    x_train_indices, y_train = RandomOverSampler().fit_sample(X=np.reshape(x_train_indices, (-1, 1)), y=y_train)\n",
    "                    x_train_indices = np.reshape(x_train_indices, (-1,))\n",
    "\n",
    "                    x_val_indices, y_val = RandomOverSampler().fit_sample(X=np.reshape(x_val_indices, (-1, 1)), y=y_val)\n",
    "                    x_val_indices = np.reshape(x_val_indices, (-1,))\n",
    "\n",
    "                    y_train = np.reshape(y_train, (-1, 1))\n",
    "                    y_val = np.reshape(y_val, (-1, 1))\n",
    "                elif sampling == 'under':\n",
    "                    x_train_indices, y_train = RandomUnderSampler().fit_sample(X=np.reshape(x_train_indices, (-1, 1)), y=y_train)\n",
    "                    x_train_indices = np.reshape(x_train_indices, (-1,))\n",
    "\n",
    "                    x_val_indices, y_val = RandomUnderSampler().fit_sample(X=np.reshape(x_val_indices, (-1, 1)), y=y_val)\n",
    "                    x_val_indices = np.reshape(x_val_indices, (-1,))\n",
    "\n",
    "                    y_train = np.reshape(y_train, (-1, 1))\n",
    "                    y_val = np.reshape(y_val, (-1, 1))\n",
    "\n",
    "\n",
    "                print(\"Hash of indices list - train: %s, validation: %s\" % (hex(hash(frozenset(x_train_indices))), hex(hash(frozenset(x_val_indices)))))\n",
    "\n",
    "        #         negatives = np.array(y_train).flatten().tolist().count(0)\n",
    "        #         positives = np.array(y_train).flatten().tolist().count(1)\n",
    "        #         print('Percentage of positives in train for fold %i: %f' % (fold, positives/negatives))\n",
    "\n",
    "        #         negatives = np.array(y_val).flatten().tolist().count(0)\n",
    "        #         positives = np.array(y_val).flatten().tolist().count(1)\n",
    "        #         print('Percentage of positives in val for fold %i: %f' % (fold, positives/negatives))\n",
    "\n",
    "                # Check wether lists are really disjoint!\n",
    "                if not set(frozenset(x_train_indices)).isdisjoint(frozenset(x_val_indices)):\n",
    "                    raise Exception\n",
    "\n",
    "                if not set(frozenset(x_train_indices)).isdisjoint(frozenset(x_test_indices)):\n",
    "                    raise Exception\n",
    "\n",
    "                x_train_indices = x_train_indices.tolist()\n",
    "                x_val_indices = x_val_indices.tolist()\n",
    "\n",
    "                for epochs in HP_EPOCHS.domain.values:\n",
    "                    for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                        for l2_l in HP_L2_L.domain.values:\n",
    "                            for loss_function in HP_LOSS_FUNCTION.domain.values:\n",
    "                                for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                                    for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                                        for decay in HP_DECAY.domain.values:\n",
    "                                            for momentum in HP_MOMENTUM.domain.values:\n",
    "                                                for num_units in HP_NUM_UNITS.domain.values:\n",
    "                                                    for num_hidden_layers in HP_NUM_HIDDEN_LAYERS.domain.values:\n",
    "                                                        for dropout_input in HP_DROPOUT_INPUT.domain.values:\n",
    "                                                            for dropout_rate in HP_DROPOUT.domain.values:\n",
    "\n",
    "                                                                hparams = {\n",
    "                                                                    HP_EPOCHS: epochs,\n",
    "                                                                    HP_BATCH_SIZE: batch_size,\n",
    "                                                                    HP_LOSS_FUNCTION: loss_function,\n",
    "                                                                    HP_OPTIMIZER: optimizer,\n",
    "                                                                    HP_LEARNING_RATE: learning_rate,\n",
    "                                                                    HP_DECAY: decay,\n",
    "                                                                    HP_MOMENTUM: momentum,\n",
    "                                                                    HP_L2_L: l2_l,\n",
    "                                                                    HP_NUM_UNITS: num_units,\n",
    "                                                                    HP_NUM_HIDDEN_LAYERS: num_hidden_layers,\n",
    "                                                                    HP_DROPOUT_INPUT: dropout_input,\n",
    "                                                                    HP_DROPOUT: dropout_rate,\n",
    "                                                                }\n",
    "\n",
    "                                                                log_dir = \"%s/%s/%s-%s-%s-fold-%i/logs\" % (EXPERIMENT_PATH, EXPERIMENT_ID, neural_net, property, sampling, fold)\n",
    "                                                                weights_path = '%s/%s/%s-%s-%s-fold-%i/weights.h5' % (EXPERIMENT_PATH, EXPERIMENT_ID, neural_net, property, sampling, fold)\n",
    "\n",
    "                                                                callbacks = [\n",
    "                                                                    TensorBoard(log_dir=log_dir),\n",
    "                                                                    hp.KerasCallback(log_dir, hparams, trial_id='%s-%s-%s-%s-%i' % (EXPERIMENT_ID, neural_net, property, sampling, fold)), # log hparams\n",
    "                                                                    ModelCheckpoint(filepath=weights_path,\n",
    "                            #                                                     monitor='val_f1_score_m',\n",
    "                                                                                monitor='val_recall',\n",
    "                                                                                verbose=0,\n",
    "                                                                                save_best_only=True,\n",
    "                                                                                mode='auto'),\n",
    "                            #                                         EarlyStopping(monitor='val_f1_score_m',\n",
    "                            #                                             verbose=0,\n",
    "                            #                                             patience=50,\n",
    "                            #                                             mode='max',\n",
    "                            #                                             restore_best_weights=True),\n",
    "                                                                                ]\n",
    "\n",
    "                                                                if neural_net == 'dnn':\n",
    "                                                                    model, history = fit_dnn_model(suppl, x_train_indices, y_train, x_val_indices, y_val, hparams, callbacks)\n",
    "                                                                elif neural_net == 'scfp':                                    \n",
    "                                                                    model, history = fit_scfp_model(suppl, x_train_indices, y_train, x_val_indices, y_val, hparams, callbacks)\n",
    "\n",
    "                                                                model.save('%s/%s/%s-%s-%s-fold-%i/model.h5' % (EXPERIMENT_PATH, EXPERIMENT_ID, neural_net, property, sampling, fold))\n",
    "\n",
    "                                                                for evaluation in ['train', 'internal']:\n",
    "                                                                    if evaluation == 'train':\n",
    "                                                                        x_indices = x_train_indices\n",
    "                                                                        y = y_train\n",
    "\n",
    "                                                                    elif evaluation == 'internal':\n",
    "                                                                        x_indices = x_val_indices\n",
    "                                                                        y = y_val\n",
    "\n",
    "                                                                    if neural_net == 'dnn':\n",
    "                                                                        x = [ AllChem.GetMorganFingerprintAsBitVect(suppl[id], 2, nBits=2048) for id in x_indices ]\n",
    "                                                                        x = np.asarray(x, dtype=np.float32)\n",
    "                                                                    elif neural_net == 'scfp':\n",
    "                                                                        x = [ mol_to_feature(suppl[id], -1, atomsize) for id in x_indices ]\n",
    "                                                                        x = np.asarray(x, dtype=np.float32)\n",
    "                                                                        x = x.reshape(-1, atomsize, featuresize, 1)\n",
    "\n",
    "                                                                    y_pred = model.predict(x)\n",
    "\n",
    "                                                                    results[neural_net][property][sampling][evaluation]['accuracy'].append(accuracy_score(y, y_pred.round()))\n",
    "                                                                    results[neural_net][property][sampling][evaluation]['roc_auc'].append(roc_auc_score(y, y_pred))\n",
    "                                                                    results[neural_net][property][sampling][evaluation]['recall'].append(recall_score(y, y_pred.round(), pos_label=1))\n",
    "                                                                    results[neural_net][property][sampling][evaluation]['specificity'].append(recall_score(y, y_pred.round(), pos_label=0))\n",
    "                                                                    results[neural_net][property][sampling][evaluation]['f1'].append(f1_score(y, y_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving results in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load random forest results\n",
    "    \n",
    "# EXPERIMENT_PATH='experiments-rf'\n",
    "# EXPERIMENT_ID='20201023-1839'\n",
    "\n",
    "# results_pickle_filename = '%s/%s/results.pickle' % (EXPERIMENT_PATH, EXPERIMENT_ID)\n",
    "    \n",
    "# # # Loading from file\n",
    "# with open(results_pickle_filename, \"rb\") as results_pickle_file:\n",
    "#     results_rf = pickle.load(results_pickle_file)\n",
    "\n",
    "# EXPERIMENT_PATH='experiments'\n",
    "# EXPERIMENT_ID='20200904-2220'\n",
    "\n",
    "results_pickle_filename = '%s/%s/results.pickle' % (EXPERIMENT_PATH, EXPERIMENT_ID)\n",
    "\n",
    "# Writing to file\n",
    "with open(results_pickle_filename, \"wb\") as results_pickle_file:\n",
    "    pickle.dump(results, results_pickle_file) \n",
    "\n",
    "# Loading from file\n",
    "# with open(results_pickle_filename, \"rb\") as results_pickle_file:\n",
    "#     results = pickle.load(results_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating best model in regards to our Single Number Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score, classification_report, make_scorer, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SNEM = 'f1'\n",
    "\n",
    "dependencies = {\n",
    "    'specificity_m': specificity_m,\n",
    "    'f1_score_m': f1_score_m,\n",
    "}\n",
    "\n",
    "for neural_net in neural_nets:\n",
    "    for property in properties:\n",
    "        \n",
    "        suppl = Chem.SDMolSupplier('data/curated/%s.sdf' % property)\n",
    "\n",
    "        x_indices, y, bad_indices = preprocess_data(suppl, range(len(suppl)), property)\n",
    "\n",
    "        print(\"Hash of indices list %s\" % (hex(hash(frozenset(x_indices)))))\n",
    "\n",
    "        # first split into training + validation (90%) and test (10%)\n",
    "        x_train_val_indices, x_test_indices, y_train_val, y_test = train_test_split(x_indices, y, test_size=0.2, stratify=y, random_state=seed)\n",
    "\n",
    "        print(\"Hash of indices list - train_val: %s, test: %s\" % (hex(hash(frozenset(x_train_val_indices))), hex(hash(frozenset(x_test_indices)))))\n",
    "        \n",
    "        negatives_full = np.array(y).flatten().tolist().count(0)\n",
    "        positives_full = np.array(y).flatten().tolist().count(1)\n",
    "        print('Entire dataset - Property: %s, size: %i, positives: %i, negatives: %i, percentage: %f' % (property, len(y), positives_full, negatives_full, (positives_full/negatives_full)))\n",
    "\n",
    "        negatives_train = np.array(y_train_val).flatten().tolist().count(0)\n",
    "        positives_train = np.array(y_train_val).flatten().tolist().count(1)\n",
    "        print('Train dataset - Property: %s, size: %i, positives: %i, negatives: %i, percentage: %f' % (property, len(y_train_val), positives_train, negatives_train, (positives_train/negatives_train)))\n",
    "\n",
    "        negatives_test = np.array(y_test).flatten().tolist().count(0)\n",
    "        positives_test = np.array(y_test).flatten().tolist().count(1)\n",
    "        print('Test dataset - Property: %s, size: %i, positives: %i, negatives: %i, percentage: %f' % (property, len(y_test), positives_test, negatives_test, (positives_test/negatives_test)))\n",
    "        \n",
    "        print('%s & %i & %f & %i & %f & %i & %f' % (\n",
    "                                                    property, len(y),\n",
    "                                                    round(positives_full/negatives_full, 3),\n",
    "                                                    len(y_train_val),\n",
    "                                                    round(positives_train/negatives_train, 3),\n",
    "                                                    len(y_test),\n",
    "                                                    round(positives_test/negatives_test, 3)\n",
    "                                                    )\n",
    "             )\n",
    "        \n",
    "        # Double checking wether our data sets are correctly disjoint\n",
    "        if not set(frozenset(x_train_val_indices)).isdisjoint(frozenset(x_test_indices)):\n",
    "            raise Exception\n",
    "        \n",
    "        for sampling in samplings:\n",
    "\n",
    "            best_model_index = results[neural_net][property][sampling]['internal'][SNEM].index(max(results[neural_net][property][sampling]['internal'][SNEM]))\n",
    "            \n",
    "            best_model = load_model('%s/%s/%s-%s-%s-fold-%i/model.h5' % (EXPERIMENT_PATH, EXPERIMENT_ID, neural_net, property, sampling, best_model_index), custom_objects=dependencies)\n",
    "\n",
    "            if neural_net == 'dnn':\n",
    "                x = [ AllChem.GetMorganFingerprintAsBitVect(suppl[id], 2, nBits=2048) for id in x_test_indices ]\n",
    "                x = np.asarray(x, dtype=np.float32)\n",
    "            elif neural_net == 'scfp':\n",
    "                x = [ mol_to_feature(suppl[id], -1, atomsize) for id in x_test_indices ]\n",
    "                x = np.asarray(x, dtype=np.float32)\n",
    "                x = x.reshape(-1, atomsize, featuresize, 1)\n",
    "\n",
    "\n",
    "        #     x = [ mol_to_feature(suppl[index], -1, atomsize) for index in x_test_indices ]\n",
    "        #     x = np.asarray(x, dtype=np.float32)\n",
    "        #     x = x.reshape(-1, atomsize, featuresize, 1)\n",
    "\n",
    "            y_pred = best_model.predict(x)\n",
    "\n",
    "\n",
    "            results[neural_net][property][sampling]['external']['accuracy'] = accuracy_score(y_test, y_pred.round())\n",
    "            results[neural_net][property][sampling]['external']['roc_auc'] = roc_auc_score(y_test, y_pred)\n",
    "            results[neural_net][property][sampling]['external']['recall'] = recall_score(y_test, y_pred.round(), pos_label=1)\n",
    "            results[neural_net][property][sampling]['external']['specificity'] = recall_score(y_test, y_pred.round(), pos_label=0)\n",
    "            results[neural_net][property][sampling]['external']['f1'] = f1_score(y_test, y_pred.round())\n",
    "\n",
    "            print('External evaluation of %s for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecificity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 results[neural_net][property][sampling]['external']['accuracy'],\n",
    "                 results[neural_net][property][sampling]['external']['f1'],\n",
    "                 results[neural_net][property][sampling]['external']['recall'],\n",
    "                 results[neural_net][property][sampling]['external']['roc_auc'],\n",
    "                 results[neural_net][property][sampling]['external']['specificity'],\n",
    "                 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out results and creating bar chart with error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "EXPERIMENT_ID = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "!mkdir -p {'%s/%s' % (EXPERIMENT_PATH, EXPERIMENT_ID)}\n",
    "\n",
    "# neural_nets = ['dnn', 'scfp']\n",
    "\n",
    "# properties = [\n",
    "#             'ahr',\n",
    "#             'are',\n",
    "#             'ar-lbd',\n",
    "#             'aromatase',\n",
    "#             'ar',\n",
    "#             'atad5',\n",
    "#             'er-lbd',\n",
    "#             'er',\n",
    "#             'hse',\n",
    "#             'mmp',\n",
    "#             'p53',\n",
    "#             'ppar'\n",
    "#             ]\n",
    "\n",
    "# samplings = ['none', 'over', 'under']\n",
    "# evaluations = ['train', 'test', 'train_std', 'test_std', 'external']\n",
    "# metrics = ['accuracy', 'roc_auc', 'recall', 'specificity', 'f1']\n",
    "\n",
    "for property in properties:\n",
    "    for sampling in samplings:\n",
    "        for neural_net in neural_nets:\n",
    "            print('Average scores of %s on train data for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecifity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 np.mean(results[neural_net][property][sampling]['train']['accuracy']),\n",
    "                 np.mean(results[neural_net][property][sampling]['train']['f1']),\n",
    "                 np.mean(results[neural_net][property][sampling]['train']['recall']),\n",
    "                 np.mean(results[neural_net][property][sampling]['train']['roc_auc']),\n",
    "                 np.mean(results[neural_net][property][sampling]['train']['specificity']),\n",
    "                 ))\n",
    "            print()\n",
    "            print('Standard deviations of %s on train data for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecifity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 np.std(results[neural_net][property][sampling]['train']['accuracy']),\n",
    "                 np.std(results[neural_net][property][sampling]['train']['f1']),\n",
    "                 np.std(results[neural_net][property][sampling]['train']['recall']),\n",
    "                 np.std(results[neural_net][property][sampling]['train']['roc_auc']),\n",
    "                 np.std(results[neural_net][property][sampling]['train']['specificity']),\n",
    "                 ))\n",
    "            print()\n",
    "            print('Average scores of %s on cross validation for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecificity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 np.mean(results[neural_net][property][sampling]['internal']['accuracy']),\n",
    "                 np.mean(results[neural_net][property][sampling]['internal']['f1']),\n",
    "                 np.mean(results[neural_net][property][sampling]['internal']['recall']),\n",
    "                 np.mean(results[neural_net][property][sampling]['internal']['roc_auc']),\n",
    "                 np.mean(results[neural_net][property][sampling]['internal']['specificity']),\n",
    "                 ))\n",
    "            print()\n",
    "            print('Standard deviations of %s on cross validation for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecificity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 np.std(results[neural_net][property][sampling]['internal']['accuracy']),\n",
    "                 np.std(results[neural_net][property][sampling]['internal']['f1']),\n",
    "                 np.std(results[neural_net][property][sampling]['internal']['recall']),\n",
    "                 np.std(results[neural_net][property][sampling]['internal']['roc_auc']),\n",
    "                 np.std(results[neural_net][property][sampling]['internal']['specificity']),\n",
    "                 ))\n",
    "            print()\n",
    "            print('Best models of %s in internal evaluation for dataset: %s with sampling: %s\\nAccuracy:\\t%i\\nf1-score:\\t%i\\nRecall:\\t%i\\nAUC:\\t%i\\nSpecificity:\\t%i'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 results[neural_net][property][sampling]['internal']['accuracy'].index(max(results[neural_net][property][sampling]['internal']['accuracy'])),\n",
    "                 results[neural_net][property][sampling]['internal']['f1'].index(max(results[neural_net][property][sampling]['internal']['f1'])),\n",
    "                 results[neural_net][property][sampling]['internal']['recall'].index(max(results[neural_net][property][sampling]['internal']['recall'])),\n",
    "                 results[neural_net][property][sampling]['internal']['roc_auc'].index(max(results[neural_net][property][sampling]['internal']['roc_auc'])),\n",
    "                 results[neural_net][property][sampling]['internal']['specificity'].index(max(results[neural_net][property][sampling]['internal']['specificity'])),\n",
    "                 ))\n",
    "            print()\n",
    "            print('Best metrics of %s in internal evaluation for dataset: %s with sampling: %s\\nAccuracy:\\t%f\\nf1-score:\\t%f\\nRecall:\\t%f\\nAUC:\\t%f\\nSpecificity:\\t%f'\n",
    "              % (neural_net,\n",
    "                 property,\n",
    "                 sampling,\n",
    "                 max(results[neural_net][property][sampling]['internal']['accuracy']),\n",
    "                 max(results[neural_net][property][sampling]['internal']['f1']),\n",
    "                 max(results[neural_net][property][sampling]['internal']['recall']),\n",
    "                 max(results[neural_net][property][sampling]['internal']['roc_auc']),\n",
    "                 max(results[neural_net][property][sampling]['internal']['specificity']),\n",
    "                 ))\n",
    "            print()\n",
    "\n",
    "        # Create lists for the plot\n",
    "        plot_metrics = ['Accuracy', 'f1-score', 'Recall', 'AUC', 'Specifity']\n",
    "        x_pos = np.arange(len(plot_metrics))\n",
    "\n",
    "        # Creating plot for CV\n",
    "\n",
    "        mean_dnn = [\n",
    "            np.mean(results['dnn'][property][sampling]['internal']['accuracy']),\n",
    "            np.mean(results['dnn'][property][sampling]['internal']['f1']),\n",
    "            np.mean(results['dnn'][property][sampling]['internal']['recall']),\n",
    "            np.mean(results['dnn'][property][sampling]['internal']['roc_auc']),\n",
    "            np.mean(results['dnn'][property][sampling]['internal']['specificity'])\n",
    "            ]\n",
    "\n",
    "        error_dnn = [\n",
    "            np.std(results['dnn'][property][sampling]['internal']['accuracy']),\n",
    "            np.std(results['dnn'][property][sampling]['internal']['f1']),\n",
    "            np.std(results['dnn'][property][sampling]['internal']['recall']),\n",
    "            np.std(results['dnn'][property][sampling]['internal']['roc_auc']),\n",
    "            np.std(results['dnn'][property][sampling]['internal']['specificity']),\n",
    "            ]\n",
    "        \n",
    "        mean_scfp = [\n",
    "            np.mean(results['scfp'][property][sampling]['internal']['accuracy']),\n",
    "            np.mean(results['scfp'][property][sampling]['internal']['f1']),\n",
    "            np.mean(results['scfp'][property][sampling]['internal']['recall']),\n",
    "            np.mean(results['scfp'][property][sampling]['internal']['roc_auc']),\n",
    "            np.mean(results['scfp'][property][sampling]['internal']['specificity'])\n",
    "            ]\n",
    "\n",
    "        error_scfp = [\n",
    "            np.std(results['scfp'][property][sampling]['internal']['accuracy']),\n",
    "            np.std(results['scfp'][property][sampling]['internal']['f1']),\n",
    "            np.std(results['scfp'][property][sampling]['internal']['recall']),\n",
    "            np.std(results['scfp'][property][sampling]['internal']['roc_auc']),\n",
    "            np.std(results['scfp'][property][sampling]['internal']['specificity']),\n",
    "            ]\n",
    "        \n",
    "        mean_rf = [\n",
    "            np.mean(results_rf[property][sampling]['test']['accuracy']),\n",
    "            np.mean(results_rf[property][sampling]['test']['f1']),\n",
    "            np.mean(results_rf[property][sampling]['test']['recall']),\n",
    "            np.mean(results_rf[property][sampling]['test']['roc_auc']),\n",
    "            np.mean(results_rf[property][sampling]['test']['specificity'])\n",
    "            ]\n",
    "\n",
    "        error_rf = [\n",
    "            results_rf[property][sampling]['test_std']['accuracy'],\n",
    "            results_rf[property][sampling]['test_std']['f1'],\n",
    "            results_rf[property][sampling]['test_std']['recall'],\n",
    "            results_rf[property][sampling]['test_std']['roc_auc'],\n",
    "            results_rf[property][sampling]['test_std']['specificity'],\n",
    "            ]\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (8, 7)\n",
    "        plt.rcParams['text.usetex'] = True\n",
    "        plt.rcParams['font.family'] = 'serif'\n",
    "        plt.rcParams['font.size'] = '28'\n",
    "\n",
    "#         color = ['pink', 'mediumaquamarine', 'lightskyblue', 'wheat', 'mediumpurple']\n",
    "        width = 0.2\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.bar(x_pos - width, mean_dnn, width, label='DNN', yerr=error_dnn, align='center', color=\"lightskyblue\", hatch='/', ecolor='black', capsize=5)\n",
    "        ax.bar(x_pos, mean_scfp, width, label='CNN', yerr=error_scfp, align='center', color=\"mediumaquamarine\", hatch='\\\\', ecolor='black', capsize=5)\n",
    "        ax.bar(x_pos + width, mean_rf, width, label='RF', yerr=error_rf, align='center', color=\"mediumpurple\", hatch='', ecolor='black', capsize=5)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(plot_metrics)\n",
    "\n",
    "        # Save the figure and show\n",
    "        plt.tight_layout()\n",
    "        plt.minorticks_on()\n",
    "        ax.tick_params(axis='x', which='minor', bottom=False)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, 'major', 'y', ls='--', lw=.5, c='k', alpha=.3)\n",
    "        plt.savefig('%s/%s/bar_plot_cv_%s_%s' % (EXPERIMENT_PATH, EXPERIMENT_ID, property, sampling), dpi=300, bbox_inches = \"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        # Creating plots for external\n",
    "\n",
    "        external_dnn = [\n",
    "            np.mean(results['dnn'][property][sampling]['external']['accuracy']),\n",
    "            np.mean(results['dnn'][property][sampling]['external']['f1']),\n",
    "            np.mean(results['dnn'][property][sampling]['external']['recall']),\n",
    "            np.mean(results['dnn'][property][sampling]['external']['roc_auc']),\n",
    "            np.mean(results['dnn'][property][sampling]['external']['specificity'])\n",
    "            ]\n",
    "\n",
    "        external_scfp = [\n",
    "            np.mean(results['scfp'][property][sampling]['external']['accuracy']),\n",
    "            np.mean(results['scfp'][property][sampling]['external']['f1']),\n",
    "            np.mean(results['scfp'][property][sampling]['external']['recall']),\n",
    "            np.mean(results['scfp'][property][sampling]['external']['roc_auc']),\n",
    "            np.mean(results['scfp'][property][sampling]['external']['specificity'])\n",
    "            ]\n",
    "        \n",
    "        external_rf = [\n",
    "            np.mean(results_rf[property][sampling]['external']['accuracy']),\n",
    "            np.mean(results_rf[property][sampling]['external']['f1']),\n",
    "            np.mean(results_rf[property][sampling]['external']['recall']),\n",
    "            np.mean(results_rf[property][sampling]['external']['roc_auc']),\n",
    "            np.mean(results_rf[property][sampling]['external']['specificity'])\n",
    "            ]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.bar(x_pos - width, external_dnn, width, label='DNN', align='center', color=\"lightskyblue\", hatch='/')\n",
    "        ax.bar(x_pos, external_scfp, width, label='CNN', align='center', color=\"mediumaquamarine\", hatch='\\\\')\n",
    "        ax.bar(x_pos + width, external_rf, width, label='RF', align='center', color=\"mediumpurple\", hatch='')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(plot_metrics)\n",
    "\n",
    "        # Save the figure and show\n",
    "        plt.tight_layout()\n",
    "        plt.minorticks_on()\n",
    "        ax.tick_params(axis='x', which='minor', bottom=False)\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, 'major', 'y', ls='--', lw=.5, c='k', alpha=.3)\n",
    "        plt.savefig('%s/%s/bar_plot_external_%s_%s' % (EXPERIMENT_PATH, EXPERIMENT_ID, property, sampling), dpi=300, bbox_inches = \"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chemception_tox21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
